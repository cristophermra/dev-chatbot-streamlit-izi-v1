{
  "name": "Notebook Name",
  "paragraphs": [
    {
      "text": "%md\n# 1. Pre-procesamiento",
      "config": {
        "editorMode": "ace/mode/markdown"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Leer el archivo CSV en un DataFrame de Spark\ndf = spark.read \\\n    .format(\"csv\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"sep\", \"|\") \\\n    .option(\"encoding\", \"ISO-8859-1\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .load(\"/notebook/data/Det_declara_2022.csv\")\ndf.count()\n\n#CAMBIAR LA RUTA DE LECTURA POR PARQUECT\n\n#df = spark.read(\"hdfs:PATH_ARCHIVO_PARKET\")",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n#from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf, split, array_intersect, lpad\nfrom pyspark.sql.types import StringType, ArrayType\nimport re\n#from unidecode import unidecode\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\ndef preprocesar_texto(texto):\n    \"\"\"\n    Preprocesa el texto:\n    - Convierte a minúsculas\n    - Elimina acentos\n    - Elimina caracteres especiales\n    - Elimina stopwords en español\n    \"\"\"\n    if texto is None:\n        return \"\"\n    \n    texto = str(texto).lower()\n    #texto = unidecode(texto)\n    texto = re.sub(r'[^a-zA-Z\\s]', ' ', texto)\n    stop_words = set(stopwords.words('spanish'))\n    palabras = texto.split()\n    palabras = [palabra for palabra in palabras if palabra not in stop_words and len(palabra) > 2]\n    return palabras\n    #return ' '.join(palabras)  # Retorna una lista de palabras clave\n\n# Registrar UDF en Spark\npreprocesar_udf = udf(preprocesar_texto, ArrayType(StringType()))",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ntext_result = preprocesar_texto(\"PESTACION DE SOLDADURA,S/M,T12-XS,\")\n#set(text_result.split())\ntext_result",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndef filtrar_por_keywords_optimizado(df1, df2):\n    \"\"\"\n    Filtra DataFrames por coincidencia de palabras clave usando PySpark.\n    \"\"\"\n    # Merge de los DataFrames en base a NUM_PARTNANDI\n    df_merged = df1.join(df2, df1.NUM_PART_NANDI == df2.NUM_PARTNANDI, \"inner\")\n    \n    # Aplicar preprocesamiento a las columnas de descripción\n    df_merged = df_merged.withColumn(\"keywords_1\", preprocesar_udf(col(\"DES_NOM_COMERC\")))\n    df_merged = df_merged.withColumn(\"keywords_2\", preprocesar_udf(col(\"DES_COMER\")))\n    \n    # Filtrar por intersección de palabras clave\n    df_merged = df_merged.withColumn(\"tiene_keyword_comun\", array_intersect(col(\"keywords_1\"), col(\"keywords_2\")))\n    df_merged = df_merged.filter(col(\"tiene_keyword_comun\").isNotNull())\n    \n    # Seleccionar columnas finales\n    resultado = df_merged.drop(\"keywords_1\", \"keywords_2\", \"tiene_keyword_comun\")\n    \n    return resultado",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf.show(5)",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams = df.withColumn(\"NUM_PARTNANDI\", lpad(col(\"NUM_PARTNANDI\"), 10, \"0\"))\ndf_dams.count()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_verde = df_dams.filter(df_dams['COD_CANAL'] == 'V')\ndf_dams_verde.count()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_doc = df_dams.filter(df_dams['COD_CANAL'] == 'D')\ndf_dams_doc.count()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_fisi = df_dams.filter(df_dams['COD_CANAL'] == 'F')\ndf_dams_fisi.count()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n\ndf_class_index = spark.read.option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"/notebook/data/INDICE_CLASIFICACION.csv\")\ndf_class_index.count()\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_class_index.printSchema()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_class_index.show(10)",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n#df_pruebas = df_class_index.filter((col(\"index\") >= 10) & (col(\"index\") <= 20))\ndf_pruebas = df_class_index\n\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import row_number\n\n# Crear una columna con números de fila\nwindow_spec = Window.orderBy(\"NUM_PART_NANDI\")  # Reemplaza \"COLUMNA_DE_ORDEN\" con una columna válida\ndf_with_row_num = df_pruebas.withColumn(\"row_num\", row_number().over(window_spec))\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_with_row_num.count()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Filtrar para obtener las filas 10 a 20\ndf_slice = df_with_row_num.filter((col(\"row_num\") >= 10) & (col(\"row_num\") <= 4020)).drop(\"row_num\")\n\n# Mostrar el resultado\ndf_slice.show()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_slice = df_slice.withColumn(\"PARTNANDI_LEN\", \n    length(trim(col(\"NUM_PART_NANDI\")))\n)\ndf_slice.show()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nfrom pyspark.sql.functions import col, length, trim, row_number\nfrom pyspark.sql.window import Window\n\n# Convert and filter using correct column name \"NUM_PART_NANDI\"\ndf_slice = df_slice.filter(\n    (col(\"PARTNANDI_LEN\") == 10) & \n    (col(\"IND_VIGENCIA\") == \"S\")  # Asumiendo que IND_DEL es equivalente a IND_VIGENCIA\n)\ndf_slice.show()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Group by using correct column name\nwindow_spec = Window.partitionBy(\"NUM_PART_NANDI\").orderBy(\"NUM_PART_NANDI\")\n\ndf_final = df_slice \\\n    .withColumn(\"row_number\", row_number().over(window_spec))\n    \ndf_final.filter(col(\"row_number\") > 1).show()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n    .filter(col(\"row_number\") == 1) \\\n    .drop(\"row_number\", \"PARTNANDI_LEN\")\n\n# Show results\nprint(\"\\nPrimeras 5 filas del resultado:\")\ndf_final.show(5)\n\n# Count records\nprint(\"\\nTotal de registros:\", df_final.count())",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n    .filter(col(\"row_number\") == 1) \\\n    .drop(\"row_number\", \"PARTNANDI_LEN\")\n\n# Show results\nprint(\"\\nPrimeras 5 filas del resultado:\")\ndf_final.show(5)\n\n# Count records\nprint(\"\\nTotal de registros:\", df_final.count())",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%md\n## FIN PRUEBAS",
      "config": {
        "editorMode": "ace/mode/markdown"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nfrom pyspark.sql.functions import col, length, trim, row_number\nfrom pyspark.sql.window import Window\n\n# Convert and filter using correct column name \"NUM_PARTNANDI\"\ndf_filtered = df_class_index.withColumn(\"PARTNANDI_LEN\", \n    length(trim(col(\"NUM_PART_NANDI\")))\n).filter(\n    (col(\"PARTNANDI_LEN\") == 10) & \n    (col(\"IND_VIGENCIA\") == \"S\")  # Asumiendo que IND_DEL es equivalente a IND_VIGENCIA\n)\n\n# Group by using correct column name\nwindow_spec = Window.partitionBy(\"NUM_PART_NANDI\").orderBy(\"NUM_PART_NANDI\")\n\ndf_final = df_filtered \\\n    .withColumn(\"row_number\", row_number().over(window_spec)) \\\n    .filter(col(\"row_number\") == 1) \\\n    .drop(\"row_number\", \"PARTNANDI_LEN\")\n\n# Show results\nprint(\"\\nPrimeras 5 filas del resultado:\")\ndf_final.show(5)\n\n# Count records\nprint(\"\\nTotal de registros:\", df_final.count())",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\ndf_dams_doc_filt = filtrar_por_keywords_optimizado(df_final, df_dams_doc)\n# Mostrar algunos resultados para verificar\nprint(\"\\nPrimeras 5 filas del resultado:\")\ndf_dams_doc_filt.show(5)\n\n# Contar registros para verificar\nprint(\"\\nTotal de registros:\", df_dams_doc_filt.count())",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_verde_filt = filtrar_por_keywords_optimizado(df_final, df_dams_verde)\n# Mostrar algunos resultados para verificar\nprint(\"\\nPrimeras 5 filas del resultado:\")\ndf_dams_verde_filt.show(5)\n\n# Contar registros para verificar\nprint(\"\\nTotal de registros:\", df_dams_verde_filt.count())",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_fisi_filt = filtrar_por_keywords_optimizado(df_final, df_dams_fisi)\n# Mostrar algunos resultados para verificar\nprint(\"\\nPrimeras 5 filas del resultado:\")\ndf_dams_fisi_filt.show(5)\n\n# Contar registros para verificar\nprint(\"\\nTotal de registros:\", df_dams_fisi_filt.count())",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%md\n**Guardar csv del canal documentario**",
      "config": {
        "editorMode": "ace/mode/markdown"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Guardar el DataFrame como CSV\ndf_dams_doc_filt.write \\\n    .mode(\"overwrite\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"encoding\", \"UTF-8\") \\\n    .csv(\"/notebook/data/cleaned/dams_cleaned_doc.csv\")\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%md\n# 2. Entrenamiento\n",
      "config": {
        "editorMode": "ace/mode/markdown"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lower, regexp_replace, udf, lit\nfrom pyspark.sql.types import StringType\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_doc_filt = spark.read.option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"/notebook/data/cleaned/dams_cleaned_doc.csv\")",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# window_spec = Window.orderBy(\"NUM_PART_NANDI\")\n# df_with_row_num = df_dams_doc_filt.withColumn(\"row_num\", row_number().over(window_spec))\n\n# df_dams_doc_filt = df_with_row_num.filter((col(\"row_num\") >= 120000) & (col(\"row_num\") <= 150000)).drop(\"row_num\")\n# df_dams_doc_filt.show()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nprint(df_dams_doc_filt.count())\nprint(df_dams_doc_filt.printSchema())",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%md\n## 2.1 Muestreo de datos",
      "config": {
        "editorMode": "ace/mode/markdown"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lower, regexp_replace, udf, lit\nfrom pyspark.sql.types import StringType\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_proc = df_dams_doc_filt.select(\"DES_COMER\", \"NUM_PARTNANDI\")",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n\ndf_dams_proc = df_dams_proc.withColumn(\"NUM_PARTNANDI\", F.lpad(col(\"NUM_PARTNANDI\").cast(\"string\"), 10, \"0\"))\ndf_dams_proc = df_dams_proc.withColumn(\"CLASE\", col(\"NUM_PARTNANDI\").substr(1, 2))\ndf_dams_proc.show(5)",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Crear un mapeo de clases a números\nclass_mapping = {clase: idx for idx, clase in enumerate(df_dams_proc.select(\"CLASE\").distinct().rdd.flatMap(lambda x: x).collect())}\nmapping_expr = F.create_map([F.lit(x) for pair in class_mapping.items() for x in pair])\n\n# Aplicar el mapeo\ndf_dams_proc = df_dams_proc.withColumn(\"CLASE_NUM\", mapping_expr[col(\"CLASE\")])\ndf_dams_proc.show(5)",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_distinct = df_dams_proc.select('CLASE_NUM').distinct()\ndf_distinct.count()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nprint(\"Distribución de clases en el dataset:\")\ndf_dams_proc.groupBy(\"CLASE_NUM\", \"CLASE\").count().orderBy(\"count\").show(60)",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_proc.printSchema()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ncapitulos = [\"62\", \"61\", \"83\", \"84\"]\n\ndf_dams_proc_filtrado = df_dams_proc.filter(col(\"CLASE\").isin(capitulos))",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_proc_filtrado.groupBy(\"CLASE_NUM\", \"CLASE\").count().orderBy(\"count\").show()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nfrom pyspark.sql.functions import col, lower, regexp_replace, udf, lit\nfrom pyspark.sql.types import StringType\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import col, lower, regexp_replace, trim, row_number, lit\n\ndef preparar_dataset_clasificacion(df, clases_seleccionadas, muestras_por_clase=100):\n    \"\"\"\n    Prepara un dataset de clasificación en PySpark.\n    \n    Parámetros:\n    -----------\n    df : DataFrame de PySpark\n        DataFrame original con columnas [DES_COMER] y CLASE_NUM.\n    clases_seleccionadas : list\n        Lista de clases a incluir en el dataset.\n    muestras_por_clase : int, opcional\n        Número de muestras a obtener por cada clase (default: 100).\n    \n    Retorna:\n    --------\n    DataFrame de PySpark\n        Dataset preparado para entrenamiento de red neuronal.\n    \"\"\"\n    # Filtrar solo las clases seleccionadas\n    df_filtrado = df.filter(col(\"CLASE_NUM\").isin(clases_seleccionadas))\n    \n    # Limpiar descripciones\n    df_filtrado = df_filtrado.withColumn(\"DESCR_LIMPIA\", trim(lower(regexp_replace(col(\"DES_COMER\"), \"\\\\s+\", \" \"))))\n    \n    # Eliminar descripciones duplicadas dentro de cada clase\n    # Agregar una columna de orden (puedes usar una columna existente o una constante)\n    window_spec = Window.partitionBy(\"CLASE_NUM\", \"DESCR_LIMPIA\").orderBy(lit(1))\n    df_unico = df_filtrado.withColumn(\"row_num\", row_number().over(window_spec)) \\\n        .filter(col(\"row_num\") == 1) \\\n        .drop(\"row_num\")\n    \n    # Preparar el dataset final con muestras equitativas\n    dataset_final = []\n    \n    for clase in clases_seleccionadas:\n        # Obtener muestras de la clase actual\n        muestras_clase = df_unico.filter(col(\"CLASE_NUM\") == clase)\n        \n        # Si hay menos muestras que las solicitadas, usar todas\n        if muestras_clase.count() <= muestras_por_clase:\n            dataset_final.append(muestras_clase)\n        else:\n            # Muestreo aleatorio estratificado\n            muestras_seleccionadas = muestras_clase.sample(fraction=1.0, seed=42).limit(muestras_por_clase)\n            dataset_final.append(muestras_seleccionadas)\n    \n    # Concatenar todas las muestras\n    dataset_preparado = dataset_final[0]\n    for df_clase in dataset_final[1:]:\n        dataset_preparado = dataset_preparado.union(df_clase)\n    \n    # Verificar la distribución de clases\n    print(\"Distribución de clases en el dataset:\")\n    dataset_preparado.groupBy(\"CLASE_NUM\").count().show()\n    \n    return dataset_preparado",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n#clases_muestreo = [0, 1, 2, 3]\n# clases_muestreo = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, \n#                     28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n#                     54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n#                     80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]\n\nclases = [16, 39, 41, 31]\n\ndataset_preparado = preparar_dataset_clasificacion(df_dams_proc_filtrado, clases_muestreo, muestras_por_clase=500)\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndataset_preparado.write \\\n    .mode(\"overwrite\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"encoding\", \"UTF-8\") \\\n    .csv(\"/notebook/data/cleaned/dams_train_test.csv\")",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%md\n# 3. Entrenamiento\n",
      "config": {
        "editorMode": "ace/mode/markdown"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_doc_filt = spark.read.option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"/notebook/data/dams_muestras_doc.csv\")",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Verificar la distribución de clases\nprint(\"Distribución de clases en el dataset:\")\ndf_dams_doc_filt.groupBy(\"CLASE_NUM\", \"CLASE\").count().show()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndf_dams_doc_filt.show(5)",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nclass_mapping = {61: 0, 84: 1, 62: 2, 85: 3}\nclass_mapping",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%md\n# 4. Entrenamiento con PyTorch",
      "config": {
        "editorMode": "ace/mode/markdown"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nclass ArancelDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n        }",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nclass TariffClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(TariffClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n        self.drop1 = nn.Dropout(p=0.3)\n        self.drop2 = nn.Dropout(p=0.2)\n        self.drop3 = nn.Dropout(p=0.1)\n        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, n_classes)\n        self.relu1 = nn.ReLU()\n        self.relu2 = nn.ReLU()\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        x = self.drop1(outputs[1])\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.drop2(x)\n\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.drop3(x)\n\n        x = self.fc3(x)\n        return x",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndataset_preparado = df_dams_doc_filt.toPandas()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ndataset_preparado",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Split de datos\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset_preparado['DES_COMER'].values, \n    dataset_preparado['CLASE_NUM'].values,\n    test_size=0.2,\n    random_state=42\n)",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nnp.array(X_train[:10]), y_train[:10]",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nnp.array(X_test[:10]), y_test[:10]",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Inicializar tokenizer y datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\ntrain_dataset = ArancelDataset(X_train, y_train, tokenizer)\ntest_dataset = ArancelDataset(X_test, y_test, tokenizer)\n\n# Dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Configuración del modelo\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = TariffClassifier(len(class_mapping))\nmodel = model.to(device)\n\n# Entrenamiento\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\nepochs = 1",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Función de entrenamiento\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for batch in tqdm(loader):\n        optimizer.zero_grad()\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(loader)",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Función de evaluación\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    \n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            _, preds = torch.max(outputs, dim=1)\n            predictions.extend(preds.cpu().tolist())\n            true_labels.extend(labels.cpu().tolist())\n    \n    return total_loss / len(loader), predictions, true_labels",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ntrain_losses = []\nval_losses = []\n# Entrenamiento del modelo\nprint(\"Iniciando entrenamiento...\")\nfor epoch in range(epochs):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss, predictions, true_labels = evaluate(model, test_loader, criterion, device)\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    print(f'Epoch {epoch + 1}/{epochs}')\n    print(f'Training Loss: {train_loss:.4f}')\n    print(f'Validation Loss: {val_loss:.4f}')",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nimport matplotlib.pyplot as plt\n\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(test_losses, label=\"Test Loss\")\nplt.xlabel(\"Épocas\")\nplt.ylabel(\"Pérdida\")\nplt.title(\"Convergencia del Modelo\")\nplt.legend()\nplt.grid()\nplt.show()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# 4. Evaluación\nprint(\"\\nEvaluación del modelo:\")\nreverse_class_mapping = {v: k for k, v in class_mapping.items()}\npredictions_mapped = [reverse_class_mapping[p] for p in predictions]\ntrue_labels_mapped = [reverse_class_mapping[t] for t in true_labels]\n\n# Crear matriz de confusión\nplt.figure(figsize=(12, 8))\ncm = confusion_matrix(true_labels_mapped, predictions_mapped)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Matriz de Confusión')\nplt.xlabel('Predicción')\nplt.ylabel('Valor Real')\nplt.show()",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# Imprimir reporte de clasificación\nprint(\"\\nReporte de Clasificación:\")\nprint(classification_report(true_labels_mapped, predictions_mapped))",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\ntorch.save(model.state_dict(), \"/notebook/data/models/class-model-nn-doc-v1.pth\")",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\nmodel = torch.load(\"/notebook/data/models/class-model-nn-doc-v1.pth\")",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%spark.pyspark\n# 5. Predicciones de ejemplo\ndef predict_text(text, model, tokenizer, device):\n    model.eval()\n    encoding = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=128,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n    \n    return reverse_class_mapping[preds.item()]\n\n# Ejemplo de predicciones\nejemplos = [\n    \"MÁQUINAS AUTOMÁTICAS PARA TRATAMIENTO O PROCESAMIENTO DE DATOS, DIGITALES\",\n    \"TEJIDOS DE ALGODÓN CON UN CONTENIDO DE ALGODÓN SUPERIOR O IGUAL AL 85% EN PESO\",\n    \"MEDICAMENTOS PARA USO TERAPÉUTICO O PROFILÁCTICO\"\n]\n\nprint(\"\\nEjemplos de predicciones:\")\nfor texto in ejemplos:\n    prediccion = predict_text(texto, model, tokenizer, device)\n    print(f\"\\nTexto: {texto}\")\n    print(f\"Capítulo predicho: {prediccion}\")",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%python\nimport json\nimport sys\nimport os\n\ndef convert_zpln_to_json(input_file, output_file=None):\n    \"\"\"\n    Convierte un archivo .zpln de Zeppelin 0.11.2 a un formato compatible con Zeppelin 0.8\n    \n    Args:\n        input_file (str): Ruta al archivo .zpln\n        output_file (str, optional): Ruta para el archivo .json de salida\n    \"\"\"\n    try:\n        # Leer el archivo .zpln\n        with open(input_file, 'r', encoding='utf-8') as f:\n            notebook = json.load(f)\n        \n        # Si no se especifica archivo de salida, crear uno basado en el nombre de entrada\n        if output_file is None:\n            output_file = os.path.splitext(input_file)[0] + '.json'\n        \n        # Guardar como JSON\n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(notebook, f, indent=2)\n            \n        print(f\"Conversión exitosa. Archivo guardado como: {output_file}\")\n        \n    except json.JSONDecodeError:\n        print(\"Error: El archivo no es un JSON válido\")\n    except Exception as e:\n        print(f\"Error durante la conversión: {str(e)}\")",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%python\nconvert_zpln_to_json(\"/notebook/INCA/NLP/training-torch_2KKB8B1SJ.zpln\", \"/notebook/INCA/NLP/training-torch-nn-v2.json\")",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%python\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    },
    {
      "text": "%python\n",
      "config": {
        "editorMode": "ace/mode/python"
      },
      "result": {}
    }
  ],
  "version": "0.9.0"
}