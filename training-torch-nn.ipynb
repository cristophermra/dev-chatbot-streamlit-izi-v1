{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3f81c9",
   "metadata": {},
   "source": [
    "%md\n",
    "# 1. Pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ad035",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Leer el archivo CSV en un DataFrame de Spark\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .option(\"encoding\", \"ISO-8859-1\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/notebook/data/Det_declara_2022.csv\")\n",
    "df.count()\n",
    "\n",
    "#CAMBIAR LA RUTA DE LECTURA POR PARQUECT\n",
    "\n",
    "#df = spark.read(\"hdfs:PATH_ARCHIVO_PARKET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa62ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "#from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, split, array_intersect, lpad\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import re\n",
    "#from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocesar_texto(texto):\n",
    "    \"\"\"\n",
    "    Preprocesa el texto:\n",
    "    - Convierte a minúsculas\n",
    "    - Elimina acentos\n",
    "    - Elimina caracteres especiales\n",
    "    - Elimina stopwords en español\n",
    "    \"\"\"\n",
    "    if texto is None:\n",
    "        return \"\"\n",
    "    \n",
    "    texto = str(texto).lower()\n",
    "    #texto = unidecode(texto)\n",
    "    texto = re.sub(r'[^a-zA-Z\\s]', ' ', texto)\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    palabras = texto.split()\n",
    "    palabras = [palabra for palabra in palabras if palabra not in stop_words and len(palabra) > 2]\n",
    "    return palabras\n",
    "    #return ' '.join(palabras)  # Retorna una lista de palabras clave\n",
    "\n",
    "# Registrar UDF en Spark\n",
    "preprocesar_udf = udf(preprocesar_texto, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96642f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "text_result = preprocesar_texto(\"PESTACION DE SOLDADURA,S/M,T12-XS,\")\n",
    "#set(text_result.split())\n",
    "text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "def filtrar_por_keywords_optimizado(df1, df2):\n",
    "    \"\"\"\n",
    "    Filtra DataFrames por coincidencia de palabras clave usando PySpark.\n",
    "    \"\"\"\n",
    "    # Merge de los DataFrames en base a NUM_PARTNANDI\n",
    "    df_merged = df1.join(df2, df1.NUM_PART_NANDI == df2.NUM_PARTNANDI, \"inner\")\n",
    "    \n",
    "    # Aplicar preprocesamiento a las columnas de descripción\n",
    "    df_merged = df_merged.withColumn(\"keywords_1\", preprocesar_udf(col(\"DES_NOM_COMERC\")))\n",
    "    df_merged = df_merged.withColumn(\"keywords_2\", preprocesar_udf(col(\"DES_COMER\")))\n",
    "    \n",
    "    # Filtrar por intersección de palabras clave\n",
    "    df_merged = df_merged.withColumn(\"tiene_keyword_comun\", array_intersect(col(\"keywords_1\"), col(\"keywords_2\")))\n",
    "    df_merged = df_merged.filter(col(\"tiene_keyword_comun\").isNotNull())\n",
    "    \n",
    "    # Seleccionar columnas finales\n",
    "    resultado = df_merged.drop(\"keywords_1\", \"keywords_2\", \"tiene_keyword_comun\")\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3dc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams = df.withColumn(\"NUM_PARTNANDI\", lpad(col(\"NUM_PARTNANDI\"), 10, \"0\"))\n",
    "df_dams.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943110f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_verde = df_dams.filter(df_dams['COD_CANAL'] == 'V')\n",
    "df_dams_verde.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375417fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_doc = df_dams.filter(df_dams['COD_CANAL'] == 'D')\n",
    "df_dams_doc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8946476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_fisi = df_dams.filter(df_dams['COD_CANAL'] == 'F')\n",
    "df_dams_fisi.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "df_class_index = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/notebook/data/INDICE_CLASIFICACION.csv\")\n",
    "df_class_index.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b971129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_class_index.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_class_index.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "#df_pruebas = df_class_index.filter((col(\"index\") >= 10) & (col(\"index\") <= 20))\n",
    "df_pruebas = df_class_index\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Crear una columna con números de fila\n",
    "window_spec = Window.orderBy(\"NUM_PART_NANDI\")  # Reemplaza \"COLUMNA_DE_ORDEN\" con una columna válida\n",
    "df_with_row_num = df_pruebas.withColumn(\"row_num\", row_number().over(window_spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_with_row_num.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Filtrar para obtener las filas 10 a 20\n",
    "df_slice = df_with_row_num.filter((col(\"row_num\") >= 10) & (col(\"row_num\") <= 4020)).drop(\"row_num\")\n",
    "\n",
    "# Mostrar el resultado\n",
    "df_slice.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_slice = df_slice.withColumn(\"PARTNANDI_LEN\", \n",
    "    length(trim(col(\"NUM_PART_NANDI\")))\n",
    ")\n",
    "df_slice.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79713cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.sql.functions import col, length, trim, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Convert and filter using correct column name \"NUM_PART_NANDI\"\n",
    "df_slice = df_slice.filter(\n",
    "    (col(\"PARTNANDI_LEN\") == 10) & \n",
    "    (col(\"IND_VIGENCIA\") == \"S\")  # Asumiendo que IND_DEL es equivalente a IND_VIGENCIA\n",
    ")\n",
    "df_slice.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c2c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Group by using correct column name\n",
    "window_spec = Window.partitionBy(\"NUM_PART_NANDI\").orderBy(\"NUM_PART_NANDI\")\n",
    "\n",
    "df_final = df_slice \\\n",
    "    .withColumn(\"row_number\", row_number().over(window_spec))\n",
    "    \n",
    "df_final.filter(col(\"row_number\") > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "    .filter(col(\"row_number\") == 1) \\\n",
    "    .drop(\"row_number\", \"PARTNANDI_LEN\")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nPrimeras 5 filas del resultado:\")\n",
    "df_final.show(5)\n",
    "\n",
    "# Count records\n",
    "print(\"\\nTotal de registros:\", df_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "    .filter(col(\"row_number\") == 1) \\\n",
    "    .drop(\"row_number\", \"PARTNANDI_LEN\")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nPrimeras 5 filas del resultado:\")\n",
    "df_final.show(5)\n",
    "\n",
    "# Count records\n",
    "print(\"\\nTotal de registros:\", df_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4924cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d159dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1eff",
   "metadata": {},
   "source": [
    "%md\n",
    "## FIN PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.sql.functions import col, length, trim, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Convert and filter using correct column name \"NUM_PARTNANDI\"\n",
    "df_filtered = df_class_index.withColumn(\"PARTNANDI_LEN\", \n",
    "    length(trim(col(\"NUM_PART_NANDI\")))\n",
    ").filter(\n",
    "    (col(\"PARTNANDI_LEN\") == 10) & \n",
    "    (col(\"IND_VIGENCIA\") == \"S\")  # Asumiendo que IND_DEL es equivalente a IND_VIGENCIA\n",
    ")\n",
    "\n",
    "# Group by using correct column name\n",
    "window_spec = Window.partitionBy(\"NUM_PART_NANDI\").orderBy(\"NUM_PART_NANDI\")\n",
    "\n",
    "df_final = df_filtered \\\n",
    "    .withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_number\") == 1) \\\n",
    "    .drop(\"row_number\", \"PARTNANDI_LEN\")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nPrimeras 5 filas del resultado:\")\n",
    "df_final.show(5)\n",
    "\n",
    "# Count records\n",
    "print(\"\\nTotal de registros:\", df_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b34d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df_dams_doc_filt = filtrar_por_keywords_optimizado(df_final, df_dams_doc)\n",
    "# Mostrar algunos resultados para verificar\n",
    "print(\"\\nPrimeras 5 filas del resultado:\")\n",
    "df_dams_doc_filt.show(5)\n",
    "\n",
    "# Contar registros para verificar\n",
    "print(\"\\nTotal de registros:\", df_dams_doc_filt.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_verde_filt = filtrar_por_keywords_optimizado(df_final, df_dams_verde)\n",
    "# Mostrar algunos resultados para verificar\n",
    "print(\"\\nPrimeras 5 filas del resultado:\")\n",
    "df_dams_verde_filt.show(5)\n",
    "\n",
    "# Contar registros para verificar\n",
    "print(\"\\nTotal de registros:\", df_dams_verde_filt.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_fisi_filt = filtrar_por_keywords_optimizado(df_final, df_dams_fisi)\n",
    "# Mostrar algunos resultados para verificar\n",
    "print(\"\\nPrimeras 5 filas del resultado:\")\n",
    "df_dams_fisi_filt.show(5)\n",
    "\n",
    "# Contar registros para verificar\n",
    "print(\"\\nTotal de registros:\", df_dams_fisi_filt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ee7c7",
   "metadata": {},
   "source": [
    "%md\n",
    "**Guardar csv del canal documentario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Guardar el DataFrame como CSV\n",
    "df_dams_doc_filt.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"/notebook/data/cleaned/dams_cleaned_doc.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d06ca8",
   "metadata": {},
   "source": [
    "%md\n",
    "# 2. Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2387e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, udf, lit\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_doc_filt = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/notebook/data/cleaned/dams_cleaned_doc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc80af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# window_spec = Window.orderBy(\"NUM_PART_NANDI\")\n",
    "# df_with_row_num = df_dams_doc_filt.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# df_dams_doc_filt = df_with_row_num.filter((col(\"row_num\") >= 120000) & (col(\"row_num\") <= 150000)).drop(\"row_num\")\n",
    "# df_dams_doc_filt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "print(df_dams_doc_filt.count())\n",
    "print(df_dams_doc_filt.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc6ec87",
   "metadata": {},
   "source": [
    "%md\n",
    "## 2.1 Muestreo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, udf, lit\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_proc = df_dams_doc_filt.select(\"DES_COMER\", \"NUM_PARTNANDI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "df_dams_proc = df_dams_proc.withColumn(\"NUM_PARTNANDI\", F.lpad(col(\"NUM_PARTNANDI\").cast(\"string\"), 10, \"0\"))\n",
    "df_dams_proc = df_dams_proc.withColumn(\"CLASE\", col(\"NUM_PARTNANDI\").substr(1, 2))\n",
    "df_dams_proc.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27455d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Crear un mapeo de clases a números\n",
    "class_mapping = {clase: idx for idx, clase in enumerate(df_dams_proc.select(\"CLASE\").distinct().rdd.flatMap(lambda x: x).collect())}\n",
    "mapping_expr = F.create_map([F.lit(x) for pair in class_mapping.items() for x in pair])\n",
    "\n",
    "# Aplicar el mapeo\n",
    "df_dams_proc = df_dams_proc.withColumn(\"CLASE_NUM\", mapping_expr[col(\"CLASE\")])\n",
    "df_dams_proc.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c4cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_distinct = df_dams_proc.select('CLASE_NUM').distinct()\n",
    "df_distinct.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50233c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "print(\"Distribución de clases en el dataset:\")\n",
    "df_dams_proc.groupBy(\"CLASE_NUM\", \"CLASE\").count().orderBy(\"count\").show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_proc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "capitulos = [\"62\", \"61\", \"83\", \"84\"]\n",
    "\n",
    "df_dams_proc_filtrado = df_dams_proc.filter(col(\"CLASE\").isin(capitulos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c69444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_proc_filtrado.groupBy(\"CLASE_NUM\", \"CLASE\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badfe46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, udf, lit\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, trim, row_number, lit\n",
    "\n",
    "def preparar_dataset_clasificacion(df, clases_seleccionadas, muestras_por_clase=100):\n",
    "    \"\"\"\n",
    "    Prepara un dataset de clasificación en PySpark.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    df : DataFrame de PySpark\n",
    "        DataFrame original con columnas [DES_COMER] y CLASE_NUM.\n",
    "    clases_seleccionadas : list\n",
    "        Lista de clases a incluir en el dataset.\n",
    "    muestras_por_clase : int, opcional\n",
    "        Número de muestras a obtener por cada clase (default: 100).\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    DataFrame de PySpark\n",
    "        Dataset preparado para entrenamiento de red neuronal.\n",
    "    \"\"\"\n",
    "    # Filtrar solo las clases seleccionadas\n",
    "    df_filtrado = df.filter(col(\"CLASE_NUM\").isin(clases_seleccionadas))\n",
    "    \n",
    "    # Limpiar descripciones\n",
    "    df_filtrado = df_filtrado.withColumn(\"DESCR_LIMPIA\", trim(lower(regexp_replace(col(\"DES_COMER\"), \"\\\\s+\", \" \"))))\n",
    "    \n",
    "    # Eliminar descripciones duplicadas dentro de cada clase\n",
    "    # Agregar una columna de orden (puedes usar una columna existente o una constante)\n",
    "    window_spec = Window.partitionBy(\"CLASE_NUM\", \"DESCR_LIMPIA\").orderBy(lit(1))\n",
    "    df_unico = df_filtrado.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "        .filter(col(\"row_num\") == 1) \\\n",
    "        .drop(\"row_num\")\n",
    "    \n",
    "    # Preparar el dataset final con muestras equitativas\n",
    "    dataset_final = []\n",
    "    \n",
    "    for clase in clases_seleccionadas:\n",
    "        # Obtener muestras de la clase actual\n",
    "        muestras_clase = df_unico.filter(col(\"CLASE_NUM\") == clase)\n",
    "        \n",
    "        # Si hay menos muestras que las solicitadas, usar todas\n",
    "        if muestras_clase.count() <= muestras_por_clase:\n",
    "            dataset_final.append(muestras_clase)\n",
    "        else:\n",
    "            # Muestreo aleatorio estratificado\n",
    "            muestras_seleccionadas = muestras_clase.sample(fraction=1.0, seed=42).limit(muestras_por_clase)\n",
    "            dataset_final.append(muestras_seleccionadas)\n",
    "    \n",
    "    # Concatenar todas las muestras\n",
    "    dataset_preparado = dataset_final[0]\n",
    "    for df_clase in dataset_final[1:]:\n",
    "        dataset_preparado = dataset_preparado.union(df_clase)\n",
    "    \n",
    "    # Verificar la distribución de clases\n",
    "    print(\"Distribución de clases en el dataset:\")\n",
    "    dataset_preparado.groupBy(\"CLASE_NUM\").count().show()\n",
    "    \n",
    "    return dataset_preparado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "#clases_muestreo = [0, 1, 2, 3]\n",
    "# clases_muestreo = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, \n",
    "#                     28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
    "#                     54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
    "#                     80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]\n",
    "\n",
    "clases = [16, 39, 41, 31]\n",
    "\n",
    "dataset_preparado = preparar_dataset_clasificacion(df_dams_proc_filtrado, clases_muestreo, muestras_por_clase=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957aa003",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "dataset_preparado.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"/notebook/data/cleaned/dams_train_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970b6b4",
   "metadata": {},
   "source": [
    "%md\n",
    "# 3. Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_doc_filt = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/notebook/data/dams_muestras_doc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Verificar la distribución de clases\n",
    "print(\"Distribución de clases en el dataset:\")\n",
    "df_dams_doc_filt.groupBy(\"CLASE_NUM\", \"CLASE\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81805ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_dams_doc_filt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "class_mapping = {61: 0, 84: 1, 62: 2, 85: 3}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63fc1b",
   "metadata": {},
   "source": [
    "%md\n",
    "# 4. Entrenamiento con PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a095336",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ad162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "class ArancelDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dad618",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "class TariffClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(TariffClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        self.drop3 = nn.Dropout(p=0.1)\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, n_classes)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        x = self.drop1(outputs[1])\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ed9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "dataset_preparado = df_dams_doc_filt.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "dataset_preparado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Split de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset_preparado['DES_COMER'].values, \n",
    "    dataset_preparado['CLASE_NUM'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53953bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "np.array(X_train[:10]), y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d81245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "np.array(X_test[:10]), y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Inicializar tokenizer y datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "train_dataset = ArancelDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = ArancelDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Configuración del modelo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TariffClassifier(len(class_mapping))\n",
    "model = model.to(device)\n",
    "\n",
    "# Entrenamiento\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6471fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Función de entrenamiento\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ec271",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Función de evaluación\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    return total_loss / len(loader), predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94609e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Entrenamiento del modelo\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, predictions, true_labels = evaluate(model, test_loader, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print(f'Training Loss: {train_loss:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b906453",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.title(\"Convergencia del Modelo\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831695aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# 4. Evaluación\n",
    "print(\"\\nEvaluación del modelo:\")\n",
    "reverse_class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "predictions_mapped = [reverse_class_mapping[p] for p in predictions]\n",
    "true_labels_mapped = [reverse_class_mapping[t] for t in true_labels]\n",
    "\n",
    "# Crear matriz de confusión\n",
    "plt.figure(figsize=(12, 8))\n",
    "cm = confusion_matrix(true_labels_mapped, predictions_mapped)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Imprimir reporte de clasificación\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(true_labels_mapped, predictions_mapped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "torch.save(model.state_dict(), \"/notebook/data/models/class-model-nn-doc-v1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43364b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "model = torch.load(\"/notebook/data/models/class-model-nn-doc-v1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# 5. Predicciones de ejemplo\n",
    "def predict_text(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "    \n",
    "    return reverse_class_mapping[preds.item()]\n",
    "\n",
    "# Ejemplo de predicciones\n",
    "ejemplos = [\n",
    "    \"MÁQUINAS AUTOMÁTICAS PARA TRATAMIENTO O PROCESAMIENTO DE DATOS, DIGITALES\",\n",
    "    \"TEJIDOS DE ALGODÓN CON UN CONTENIDO DE ALGODÓN SUPERIOR O IGUAL AL 85% EN PESO\",\n",
    "    \"MEDICAMENTOS PARA USO TERAPÉUTICO O PROFILÁCTICO\"\n",
    "]\n",
    "\n",
    "print(\"\\nEjemplos de predicciones:\")\n",
    "for texto in ejemplos:\n",
    "    prediccion = predict_text(texto, model, tokenizer, device)\n",
    "    print(f\"\\nTexto: {texto}\")\n",
    "    print(f\"Capítulo predicho: {prediccion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81cece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def convert_zpln_to_json(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Convierte un archivo .zpln de Zeppelin 0.11.2 a un formato compatible con Zeppelin 0.8\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Ruta al archivo .zpln\n",
    "        output_file (str, optional): Ruta para el archivo .json de salida\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Leer el archivo .zpln\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            notebook = json.load(f)\n",
    "        \n",
    "        # Si no se especifica archivo de salida, crear uno basado en el nombre de entrada\n",
    "        if output_file is None:\n",
    "            output_file = os.path.splitext(input_file)[0] + '.json'\n",
    "        \n",
    "        # Guardar como JSON\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(notebook, f, indent=2)\n",
    "            \n",
    "        print(f\"Conversión exitosa. Archivo guardado como: {output_file}\")\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: El archivo no es un JSON válido\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante la conversión: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "convert_zpln_to_json(\"/notebook/INCA/NLP/training-torch_2KKB8B1SJ.zpln\", \"/notebook/INCA/NLP/training-torch-nn-v2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ab1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06955a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
